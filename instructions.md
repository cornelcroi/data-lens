# ðŸ“˜ MASTER SPEC â€“ `data-lens` (FastMCP MCP Server)

Version: **1.0**
Audience: **Product + Junior/Mid Engineers**
Goal: Implement an MCP server that lets an LLM analyze spreadsheet data (Excel/CSV/Parquet) via DuckDB and SQL, using **FastMCP**.

---

## 1. What is `data-lens`?

`data-lens` is an MCP server that lets an AI assistant:

1. Load a spreadsheet file (Excel/CSV/Parquet)
2. Convert it into one or more **DuckDB** tables
3. Inspect schema: tables, columns, sample values
4. Preview data rows
5. Run SQL queries (generated by the LLM)
6. Reset/clear the database when needed

The **LLM** does:

* â€œUnderstandâ€ the schema
* Convert natural language â†’ SQL
* Call `run_sql`
* Use the results to answer the user

The **server** does:

* File loading
* DuckDB table creation
* Schema extraction
* Row preview
* SQL execution (with safety guard)
* Simple state tracking (uploaded files)

Everything is wrapped in a **FastMCP** app with tools and a guiding **Text-to-SQL prompt**. ([GitHub][1])

---

## 2. Tech Stack

* **Python** 3.10+
* **FastMCP** (`fastmcp` package) â€“ official Python MCP SDK ([PyPI][2])
* **DuckDB** (`duckdb`)
* **pandas** (data loading)
* **openpyxl** (Excel engine)
* **pyarrow** (Parquet support)

---

## 3. Install Everything

From inside your project folder:

```bash
pip install fastmcp duckdb pandas openpyxl pyarrow
```

Later, to test with Claude Desktop, youâ€™ll use:

```bash
fastmcp dev server.py
# and/or
fastmcp install server.py
```

([PyPI][2])

---

## 4. Project Layout

For an MVP and junior-friendly structure, weâ€™ll keep **everything in one file**: `server.py`.

You can refactor into modules later if needed.

```
data-lens/
  server.py
```

---

## 5. `server.py` â€“ Full Implementation

Below is the **complete** `server.py` with:

* FastMCP app
* DuckDB engine
* Helper functions
* Tools
* Prompt

Your engineers can copy this and then adjust as needed.

---

### 5.1 Imports and App Setup

```python
# server.py

from typing import List, Dict, Any
from pathlib import Path
import re

import duckdb
import pandas as pd

from fastmcp import FastMCP

# Create FastMCP server
mcp = FastMCP("data-lens", dependencies=["duckdb", "pandas", "openpyxl", "pyarrow"])
```

---

### 5.2 Simple DuckDB Engine + State

We keep state in module-level variables:

```python
# In-memory DuckDB connection
con = duckdb.connect(database=":memory:")

# Track uploaded files (for list_files)
ACTIVE_FILES: List[str] = []
```

---

### 5.3 Utility: Sanitize Table Names

We must convert file names / sheet names into valid SQL identifiers.

```python
def sanitize_table_name(source: str) -> str:
    """Convert a file or sheet name into a safe SQL table name."""
    name = Path(source).stem  # remove extension
    name = re.sub(r"[^a-zA-Z0-9_]", "_", name)
    return name.lower()
```

---

### 5.4 Utility: Reset Database

Used by `clear_all` and when we want to fully reset:

```python
def reset_database():
    """Reset DuckDB connection and clear active files."""
    global con, ACTIVE_FILES
    con.close()
    con = duckdb.connect(database=":memory:")
    ACTIVE_FILES = []
```

---

### 5.5 Loading a File into DuckDB

Handles `.xlsx`, `.csv`, `.tsv`, `.parquet`.

```python
def load_file_to_duckdb(file_path: str) -> List[str]:
    """
    Load a spreadsheet file into DuckDB.
    Returns a list of created table names.
    Replaces the previous dataset (single-file mode).
    """
    global con, ACTIVE_FILES

    # Reset DB for now (single active dataset)
    con.close()
    con = duckdb.connect(database=":memory:")

    ext = Path(file_path).suffix.lower().lstrip(".")
    tables: List[str] = []

    if ext == "xlsx":
        excel = pd.ExcelFile(file_path)
        for sheet in excel.sheet_names:
            df = excel.parse(sheet)
            table = sanitize_table_name(sheet)
            con.execute("CREATE TABLE {} AS SELECT * FROM df".format(table))
            tables.append(table)

    elif ext in ("csv", "tsv"):
        df = pd.read_csv(file_path)
        table = sanitize_table_name(file_path)
        con.execute("CREATE TABLE {} AS SELECT * FROM df".format(table))
        tables.append(table)

    elif ext == "parquet":
        table = sanitize_table_name(file_path)
        con.execute(f"CREATE TABLE {table} AS SELECT * FROM parquet_scan('{file_path}')")
        tables.append(table)

    else:
        raise ValueError(f"Unsupported file format: {ext}")

    # Track only this file (single-file mode)
    ACTIVE_FILES = [file_path]

    return tables
```

---

### 5.6 Schema Inspector â€“ Columns + Sample Values

```python
def get_table_schema(table: str) -> Dict[str, Any]:
    """Return columns + sample values for a table."""
    # DESCRIBE returns: [ (column_name, column_type, ...), ... ]
    rows = con.execute(f"DESCRIBE {table}").fetchall()
    columns = {r[0]: r[1] for r in rows}

    # Get first 5 rows as samples
    df = con.execute(f"SELECT * FROM {table} LIMIT 5").fetchdf()
    sample_values = {col: df[col].astype(str).tolist() for col in df.columns}

    return {
        "columns": columns,
        "sample_values": sample_values,
    }
```

---

### 5.7 SQL Safety Guard

```python
def is_safe_sql(sql: str) -> bool:
    """
    Very simple SQL safety filter.
    Blocks destructive operations.
    """
    forbidden = ["DROP", "DELETE", "UPDATE", "ALTER", "CREATE TABLE"]
    upper = sql.upper()
    return not any(word in upper for word in forbidden)
```

---

## 6. FastMCP Prompt â€“ Text-to-SQL Guide

FastMCP has a `@mcp.prompt()` decorator you can use to expose prompts to the client. ([DataCamp][3])

We create one **prompt** that explains how the LLM should work with `data-lens`.

```python
@mcp.prompt("text_to_sql_guide")
def text_to_sql_guide() -> str:
    """
    Prompt that teaches the LLM how to use data-lens for Text-to-SQL with DuckDB.
    """
    return """
You are a SQL reasoning assistant for the data-lens MCP server.

Your job is to analyze spreadsheet data by:
1. Inspecting schema using the get_schema tool.
2. Listing tables using list_tables.
3. Listing columns using list_columns.
4. Previewing example rows via preview_rows.
5. Writing correct DuckDB SQL using column names returned from get_schema.
6. Executing SQL using the run_sql tool.
7. Returning clear answers to the user.

Workflow:
1. If the user uploads a file, call load_file.
2. Always call get_schema before writing SQL, unless you already know the schema.
3. Use column names EXACTLY as returned by get_schema, list_columns, or preview_rows.
4. If multiple tables exist, choose the one matching the user's question.
5. If a SQL error occurs, correct the query and retry automatically using run_sql.

Rules:
- Do NOT guess column names.
- Do NOT hallucinate tables.
- Do NOT use DROP, DELETE, UPDATE, ALTER, CREATE TABLE.
- Use DuckDB syntax.
- Use EXTRACT(month FROM date_column) or similar functions for date logic.
- Use CAST(column AS DOUBLE/DATE) if needed.
- Use preview_rows to understand the data content.
- Use list_tables to discover available tables.
- Use list_columns when the user asks about columns.
- Use clear_all to reset the database when needed.

When you answer the user:
- First, ensure your SQL is correct and has been executed via run_sql.
- Then summarize the results in natural language.
"""
```

Clients like Claude Desktop can explicitly apply this prompt as a â€œguideâ€ when interacting with your server.

---

## 7. Tools (FastMCP)

Now we define all tools with `@mcp.tool()`.

FastMCP automatically infers schemas from function signatures & docstrings. ([GitHub][1])

### 7.1 `load_file`

```python
@mcp.tool()
def load_file(file_path: str) -> Dict[str, Any]:
    """
    Load a spreadsheet file (Excel/CSV/Parquet) into DuckDB.
    Replaces any previously loaded dataset (single-file mode).
    Returns the list of created table names.
    """
    tables = load_file_to_duckdb(file_path)
    return {
        "file_path": file_path,
        "tables": tables,
        "mode": "single_file"
    }
```

---

### 7.2 `list_files`

```python
@mcp.tool()
def list_files() -> Dict[str, Any]:
    """
    List all files loaded into data-lens in this session.
    """
    return {"files": ACTIVE_FILES}
```

---

### 7.3 `list_tables`

```python
@mcp.tool()
def list_tables() -> Dict[str, Any]:
    """
    List all DuckDB tables currently available.
    """
    rows = con.execute(
        "SELECT table_name FROM information_schema.tables"
    ).fetchall()
    return {"tables": [r[0] for r in rows]}
```

---

### 7.4 `list_columns`

```python
@mcp.tool()
def list_columns(table: str) -> Dict[str, Any]:
    """
    List columns for a specific table, with their types.
    """
    rows = con.execute(f"DESCRIBE {table}").fetchall()
    return {
        "table": table,
        "columns": [{"name": r[0], "type": r[1]} for r in rows]
    }
```

---

### 7.5 `preview_rows`

```python
@mcp.tool()
def preview_rows(table: str, limit: int = 5) -> Dict[str, Any]:
    """
    Return the first `limit` rows of a table, as strings.
    """
    df = con.execute(f"SELECT * FROM {table} LIMIT {limit}").fetchdf()
    return {
        "table": table,
        "limit": limit,
        "columns": df.columns.tolist(),
        "rows": df.astype(str).values.tolist()
    }
```

---

### 7.6 `get_schema`

```python
@mcp.tool()
def get_schema() -> Dict[str, Any]:
    """
    Return schema info for all tables:
    - table names
    - columns + types
    - sample values
    """
    rows = con.execute(
        "SELECT table_name FROM information_schema.tables"
    ).fetchall()
    tables = [r[0] for r in rows]

    schemas: Dict[str, Any] = {}
    for t in tables:
        schemas[t] = get_table_schema(t)

    return {
        "tables": tables,
        "schemas": schemas
    }
```

---

### 7.7 `run_sql`

```python
@mcp.tool()
def run_sql(sql: str) -> Dict[str, Any]:
    """
    Execute a SQL query against the current DuckDB database.
    Blocks destructive statements like DROP/DELETE/UPDATE.
    Returns columns and rows as strings.
    """
    if not is_safe_sql(sql):
        return {"error": "Unsafe SQL detected. Destructive statements are not allowed."}

    try:
        df = con.execute(sql).fetchdf()
        return {
            "columns": df.columns.tolist(),
            "rows": df.astype(str).values.tolist()
        }
    except Exception as e:
        # LLM is expected to correct SQL and retry
        return {"error": str(e)}
```

---

### 7.8 `clear_all`

```python
@mcp.tool()
def clear_all() -> Dict[str, Any]:
    """
    Reset the DuckDB database and remove all loaded files.
    """
    reset_database()
    return {"status": "OK", "message": "Database reset and all files cleared."}
```

---

### 7.9 FastMCP Entrypoint

At the bottom of `server.py`:

```python
if __name__ == "__main__":
    # Run with: fastmcp dev server.py
    mcp.run()
```

---

## 8. How the LLM is Expected to Use the Tools (Examples)

### Example 1 â€“ User uploads a file and asks a question

**User:**

> Iâ€™ve uploaded `sales.xlsx`. What is the average amount?

**LLM typical call sequence:**

1. `load_file` (if needed â€“ some clients handle upload separately)
2. `get_schema` to understand tables and columns
3. Build SQL e.g. `SELECT AVG(amount) FROM sales;`
4. `run_sql(sql="SELECT AVG(amount) FROM sales;")`
5. Use result to answer.

---

### Example 2 â€“ Listing files and tables

**User:**

> What files and tables do you see?

LLM calls:

```jsonc
// 1. List files
{ "tool": "list_files", "arguments": {} }

// 2. List tables
{ "tool": "list_tables", "arguments": {} }
```

---

### Example 3 â€“ Show me a few rows

**User:**

> Show me a few rows of the main table.

LLM might:

1. Call `list_tables` to get table name (e.g. `"sales"`)
2. Call:

   ```json
   { "tool": "preview_rows", "arguments": { "table": "sales", "limit": 3 } }
   ```

---

### Example 4 â€“ Reset everything

**User:**

> Clear everything and start over.

LLM:

```json
{ "tool": "clear_all", "arguments": {} }
```

---

## 9. Testing Guide (For Engineers)

### 9.1 Manual Testing (without LLM)

You can test tools by calling functions directly in a Python REPL:

```python
from server import load_file, list_tables, get_schema, run_sql

load_file("examples/sales.xlsx")
list_tables()
get_schema()
run_sql("SELECT COUNT(*) FROM sales;")
```

### 9.2 MCP Testing with FastMCP CLI

```bash
fastmcp dev server.py
```

Then connect this server from:

* Claude Desktop (via MCP settings)
* MCP Inspector (if used)
* Any MCP-compatible client

---

## 10. Engineering Roadmap (Suggested)

**Day 1**

* Set up repo with `server.py`
* Install `fastmcp`, `duckdb`, `pandas`, `openpyxl`, `pyarrow`
* Implement DuckDB connection + `load_file_to_duckdb`
* Implement `load_file` tool

**Day 2**

* Implement `list_files`, `list_tables`, `list_columns`, `preview_rows`
* Test loading files and basic exploration

**Day 3**

* Implement `get_table_schema`, `get_schema`
* Implement `run_sql` with `is_safe_sql`
* Test simple SQL queries from Python

**Day 4**

* Add `clear_all` and `reset_database`
* Implement `text_to_sql_guide` prompt via `@mcp.prompt`
* Run `fastmcp dev server.py` and test with a client

**Day 5**

* Edge cases: unsupported formats, empty tables, missing columns
* Add comments & docstrings
* QA with a few real Excel files

---

## 11. Summary

With this single file `server.py` and the instructions above, your team can:

* Build a fully working **FastMCP** server
* Ingest spreadsheets into DuckDB
* Provide schema + preview tools
* Let the *calling LLM* handle Text-to-SQL
* Provide a robust, resettable environment
* Rely on MCP prompts to guide the LLM behavior

If youâ€™d like, I can next:

* Turn this into a **GitHub-ready README.md**
* Propose a **shorter PM-facing summary**
* Add **optional multi-file mode** design for later versions.

[1]: https://github.com/jlowin/fastmcp?utm_source=chatgpt.com "jlowin/fastmcp: ðŸš€ The fast, Pythonic way to build MCP ..."
[2]: https://pypi.org/project/fastmcp/2.0.0/?utm_source=chatgpt.com "fastmcp 2.0.0"
[3]: https://www.datacamp.com/tutorial/building-mcp-server-client-fastmcp?utm_source=chatgpt.com "Building an MCP Server and Client with FastMCP 2.0"
